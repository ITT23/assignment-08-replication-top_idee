{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25242ec-ab4e-4deb-bc2c-605c6d7febfe",
   "metadata": {},
   "source": [
    "# Implementation of the algorithm used in PianoText\n",
    "To get a mapping between a piano and common letters we have to:\n",
    "- read in midi files of melodies\n",
    "- build uni-grams of the appearance of notes\n",
    "- build bi-grams of the transition of notes\n",
    "- read in a text corpus\n",
    "- build uni-grams of the letters\n",
    "- build bi-grams of the letters\n",
    "- apply the algorithm described in PianoText\n",
    "\n",
    "The link to the text dataset in the PianoText paper is dead so we used the 'AUTHORS'-Folder from the 'Classic Literature in ASCII' dataset by Myles O'Neill (https://www.kaggle.com/datasets/mylesoneill/classic-literature-in-ascii?resource=download). The piano dataset used in the paper is not publicly available, other public piano datasets are not generated by side-reading (as in the paper) and consist of many notes played at the same time, so they are not suitable for generating bi-grams of transitions. We used the 'Melody' lines from the 'POP 909' dataset (from Wang, Z. et al. (2020). Pop909: A pop-song dataset for music arrangement generation. arXiv preprint arXiv:2008.07142. | https://github.com/music-x-lab/POP909-Dataset) as a proxy dataset for the proof of concept of the algorithm. These melody lines are good as a proxy dataset because there is only one note at a time like in sight reading. However, these melodies are intended to be performed by a singer, so the mapping is not the same as for a piano. So, we used this notebook as a proof of concept for the implementation of the algorithm but used the mapping presented in the paper for the actual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80ec48f-dcfe-4acb-95d4-0da5469589dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mido\n",
    "import os\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "lower_case_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb44e0f-c76d-473f-9587-2c3dbf2e858e",
   "metadata": {},
   "source": [
    "# read in midi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9637b75-aa57-42e8-9087-582cdd538817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "melody_data = []\n",
    "for root, _, files in os.walk('POP909'):\n",
    "    for file in files:\n",
    "        name = file.split('.')[0]\n",
    "        if(len(name) == 3):\n",
    "            mid = mido.MidiFile(f'POP909/{name}/{file}')\n",
    "            for track in mid.tracks:\n",
    "                melody = []\n",
    "\n",
    "                if(track.name == 'MELODY'):\n",
    "                    for msg in track:\n",
    "                        if hasattr(msg, 'velocity'):\n",
    "                            if(msg.velocity > 0):\n",
    "                                melody.append(msg)\n",
    "                    melody_data.append(melody)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6683f7-cedc-4b87-b52e-a724375b0cda",
   "metadata": {},
   "source": [
    "# create uni-grams for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca3132e-d406-446e-a3df-0a43b0f0bec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('71', 0.06288478878428559), ('74', 0.062238424422231056), ('72', 0.06017652210727709), ('69', 0.05911002090988711), ('73', 0.0562143085678828), ('75', 0.053606228366992754), ('76', 0.053286278007775766), ('67', 0.05283382295433759), ('70', 0.049366078151915016), ('77', 0.04689373446705643), ('68', 0.04103767334684235), ('78', 0.03901778471542193), ('79', 0.036930027825985784), ('66', 0.036855695924349514), ('65', 0.03396321540415548), ('64', 0.03266402303642586), ('80', 0.026694848152852245), ('62', 0.025728533431580717), ('63', 0.023799135810847933), ('81', 0.022406220610620413), ('60', 0.016718214224540517), ('82', 0.016527536737734428), ('61', 0.013347424076426122), ('83', 0.012044999886886237), ('84', 0.010888007678808622), ('59', 0.010858921282516167), ('58', 0.007759604166464678), ('57', 0.006780362157952059), ('86', 0.005115973925661634), ('85', 0.004705532555757006), ('55', 0.0038394043106039304), ('56', 0.003047607967087127), ('88', 0.002902175985624857), ('87', 0.0022881298416730497), ('89', 0.002000497700558782), ('53', 0.0011020512373029802), ('54', 0.000995401117563982), ('91', 0.0008693600669633479), ('90', 0.0006883780455880785), ('52', 0.0005688006386079897), ('92', 0.0003296458246478122), ('51', 0.0002682412102526315), ('50', 0.00020360477404717813), ('93', 0.00018421384318554213), ('48', 8.725918887736206e-05), ('96', 4.847732715409003e-05), ('95', 2.585457448218135e-05), ('49', 2.585457448218135e-05), ('98', 1.6159109051363343e-05), ('47', 1.6159109051363343e-05), ('94', 1.6159109051363343e-05), ('43', 6.463643620545337e-06), ('42', 6.463643620545337e-06), ('46', 6.463643620545337e-06), ('44', 3.2318218102726686e-06)]\n"
     ]
    }
   ],
   "source": [
    "uni_grams = {}\n",
    "for melody in melody_data:\n",
    "    for note in melody:\n",
    "        # we only need the notes played\n",
    "        if(note.type == 'note_on'):\n",
    "            if f'{note.note}' in uni_grams:\n",
    "                uni_grams[f'{note.note}'] = uni_grams[f'{note.note}'] + 1\n",
    "            else:\n",
    "                uni_grams[f'{note.note}'] = 1\n",
    "number_of_notes = 0\n",
    "#generate probabilities\n",
    "for apperences in uni_grams.values():\n",
    "    number_of_notes += apperences\n",
    "for key in uni_grams.keys():\n",
    "    uni_grams[key] = uni_grams[key] / number_of_notes\n",
    "# sort the uni-grams\n",
    "# source for sorting the code: https://www.freecodecamp.org/news/sort-dictionary-by-value-in-python/\n",
    "uni_grams = sorted(uni_grams.items(), key=lambda x:x[1], reverse=True)\n",
    "print(uni_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d543c-13c5-4ead-a7c7-1c4f9da77ffe",
   "metadata": {},
   "source": [
    "# create bi grams for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2f3a5e-5ca1-4aeb-b445-b8e648ffecdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[71, 69]', 0.013058955478617058), ('[74, 72]', 0.012834641829619154), ('[76, 74]', 0.012802132605126705), ('[69, 67]', 0.011882121551990378), ('[69, 71]', 0.011374977649908162), ('[75, 73]', 0.011267697209083077), ('[72, 74]', 0.01092309942946311), ('[73, 71]', 0.010812568066188782), ('[67, 69]', 0.010789811609044067), ('[74, 76]', 0.010376944457989954), ('[71, 73]', 0.010097365127354888), ('[73, 75]', 0.010035597600819233), ('[72, 70]', 0.009834040408966044), ('[77, 75]', 0.009567464768127957), ('[70, 72]', 0.00917085222932007), ('[78, 76]', 0.008715723086425773), ('[67, 65]', 0.008702719396628793), ('[75, 77]', 0.008673461094585588), ('[70, 68]', 0.00854992604151428), ('[79, 77]', 0.008036280294533574), ('[65, 67]', 0.007971261845548675), ('[68, 70]', 0.007854228637375855), ('[76, 78]', 0.00727556444141025), ('[68, 66]', 0.0071617821556866764), ('[77, 79]', 0.006492092131142211), ('[66, 64]', 0.006466084751548252), ('[66, 68]', 0.006436826449505047), ('[80, 78]', 0.005822402106597747), ('[64, 66]', 0.005682612441280213), ('[64, 62]', 0.0056663578290339885), ('[65, 63]', 0.005594837535150599), ('[81, 79]', 0.005572081078005884), ('[74, 71]', 0.0052859999024723265), ('[62, 64]', 0.005237236065733652), ('[72, 69]', 0.005074689943271403), ('[63, 65]', 0.004970660424895564), ('[79, 81]', 0.004886136441215195), ('[69, 72]', 0.004827619837128786), ('[71, 74]', 0.004678077404463517), ('[78, 80]', 0.004326977779945059), ('[82, 80]', 0.0040896604411501765), ('[77, 74]', 0.004073405828903951), ('[73, 70]', 0.004050649371759237), ('[75, 72]', 0.004027892914614522), ('[62, 60]', 0.003884852326847743), ('[67, 64]', 0.003777571886022659), ('[76, 73]', 0.0037483135839794543), ('[78, 75]', 0.0036897969798930447), ('[74, 77]', 0.0036832951349945547), ('[72, 75]', 0.00364103314315437)]\n"
     ]
    }
   ],
   "source": [
    "bi_grams = {}\n",
    "notes_of_melody = []\n",
    "for melody in melody_data:\n",
    "    melody_notes = []\n",
    "    for note in melody:\n",
    "        # we only need the notes played\n",
    "        if(note.type == 'note_on'):\n",
    "            melody_notes.append(note.note)\n",
    "    notes_of_melody.append(melody_notes)\n",
    "for melody in notes_of_melody:\n",
    "    # create note transitions\n",
    "    for i in range(len(melody)):\n",
    "        if(i + 1 < len(melody) - 1):\n",
    "            bi_gram = [melody[i], melody[i + 1]]\n",
    "            if f'{bi_gram}' in bi_grams:\n",
    "                bi_grams[f'{bi_gram}'] = bi_grams[f'{bi_gram}'] + 1\n",
    "            else:\n",
    "                bi_grams[f'{bi_gram}'] = 1\n",
    "# #generate probabilities\n",
    "number_of_notes = 0\n",
    "for apperences in bi_grams.values():\n",
    "    number_of_notes += apperences\n",
    "for key in bi_grams.keys():\n",
    "    bi_grams[key] = bi_grams[key] / number_of_notes\n",
    "# sort the bi-grams\n",
    "# source: https://www.freecodecamp.org/news/sort-dictionary-by-value-in-python/\n",
    "bi_grams = sorted(bi_grams.items(), key=lambda x:x[1], reverse=True)\n",
    "bi_grams_transitions = []\n",
    "# remove transitions form one note to its self\n",
    "for bi_gram in bi_grams:\n",
    "    note_one_two = bi_gram[0].split(',')\n",
    "    note_one = note_one_two[0]\n",
    "    note_two = note_one_two[1]\n",
    "    note_one = note_one.replace('[', '')\n",
    "    note_two = note_two.replace(']', '')\n",
    "    note_two = note_two.replace(' ', '')\n",
    "    note_one = int(note_one)\n",
    "    note_two = int(note_two)\n",
    "    if(note_one != note_two):\n",
    "        bi_grams_transitions.append(bi_gram)\n",
    "print(bi_grams_transitions[: 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96da9c9-da61-482b-a234-51022c374566",
   "metadata": {},
   "source": [
    "# Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6333b43a-20a8-443b-b73b-d721b1a1ab29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_files_safe = []\n",
    "for root, dirs, files in os.walk('AUTHORS'):\n",
    "    for dir in dirs:\n",
    "        for root, _, text_files in os.walk(f'AUTHORS/{dir}'):\n",
    "            for text_file in text_files:\n",
    "                if(text_file[-4:] == '.txt'):\n",
    "                    with open(f'AUTHORS/{dir}/{text_file}') as f:\n",
    "                        content = f.readlines()\n",
    "                        text_files_safe.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6386a8-067f-4d81-a214-568f88e99ca2",
   "metadata": {},
   "source": [
    "# Create text uni grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c297b4-f2e2-462d-87dc-fd032f6f18f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e', 0.12730031130753489], ['t', 0.09313182470104242], ['a', 0.07975326393940883], ['o', 0.0781903267933764], ['n', 0.07103014646638407], ['i', 0.06713406315262423], ['s', 0.06340980528350634], ['h', 0.06288542979899193], ['r', 0.059192868739713286], ['d', 0.042241907443057496], ['l', 0.03976524597860653], ['u', 0.029218271782903184], ['c', 0.025396137918185492], ['m', 0.024858787560078248], ['f', 0.0233441433333846], ['w', 0.02187000710187403], ['y', 0.019239978233293417], ['g', 0.019238600111126495], ['p', 0.01725376699065195], ['b', 0.014793012574617069], ['v', 0.009483342439446594], ['k', 0.006911458597610861], ['x', 0.0016158775624660628], ['j', 0.0011128776324130932], ['q', 0.0010200156557610338], ['z', 0.0006085289019414326]]\n"
     ]
    }
   ],
   "source": [
    "letters = []\n",
    "for t in text_files_safe:\n",
    "    for line in t:\n",
    "        for char in line:\n",
    "            # we only are intrested in the apperence of lower case letters\n",
    "            if(char in lower_case_letters):\n",
    "                letters.append(char)\n",
    "# generate letter frequencys\n",
    "uni_gram_distribution = FreqDist(letters)\n",
    "letter_counts = []\n",
    "for k, v in uni_gram_distribution.items():\n",
    "    letter_counts.append([k, v])\n",
    "# calculate the probabilities from the frequencys\n",
    "apperences = 0\n",
    "for letter in letter_counts:\n",
    "    apperences += letter[1]\n",
    "for letter in letter_counts:\n",
    "    letter[1] = letter[1] / apperences\n",
    "letter_counts = sorted(letter_counts, key=lambda x:x[1], reverse=True)\n",
    "print(letter_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8b835-a660-49e7-ac5e-897308c93ccb",
   "metadata": {},
   "source": [
    "# Create text bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3da13da-2b9a-45f6-b9bb-b38f245a24c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t - h', 0.04021206397391681], ['h - e', 0.03700136501874384], ['i - n', 0.023551434017478914], ['a - n', 0.02183930283762451], ['e - r', 0.021357721003514074], ['r - e', 0.018413084698847414], ['n - d', 0.01700041780809704], ['o - n', 0.015126513020407404], ['a - t', 0.014308748103900955], ['e - n', 0.013785979863528804], ['o - u', 0.01353493069351734], ['h - a', 0.013506130888021486], ['e - d', 0.012238805226071762], ['i - t', 0.0121202505007849], ['i - s', 0.012094365189586896], ['o - r', 0.011835588774825212], ['h - i', 0.011530468065866804], ['e - s', 0.011426217371804914], ['o - f', 0.011317556587687067], ['t - o', 0.011274471925270832], ['n - g', 0.010955890854489463], ['a - r', 0.01035479557986261], ['a - s', 0.009982699024967437], ['t - e', 0.009839198529407536], ['s - t', 0.009786104879994739], ['s - e', 0.009444783083968306], ['v - e', 0.009118551465655081], ['l - e', 0.008880349079719664], ['m - e', 0.00869017832678499], ['t - i', 0.00848790858765332]]\n"
     ]
    }
   ],
   "source": [
    "letter_bi_grams = []\n",
    "for t in text_files_safe:\n",
    "    for line in t:\n",
    "        # get word 'transitions'\n",
    "        for i in range(len(line)):\n",
    "            if(i + 1 < len(line) - 1):\n",
    "                if(line[i] in lower_case_letters and line[i + 1] in lower_case_letters):\n",
    "                    letter_bi_grams.append(f'{line[i]} - {line[i + 1]}')\n",
    "# generate 'transition' frequencys\n",
    "bi_gram_distribution = FreqDist(letter_bi_grams)\n",
    "letter_counts_bi_grams = []\n",
    "for k, v in bi_gram_distribution.items():\n",
    "    letter_counts_bi_grams.append([k, v])\n",
    "# calculate the probabilities from the frequencys\n",
    "apperences = 0\n",
    "for letter in letter_counts_bi_grams:\n",
    "    apperences += letter[1]\n",
    "for letter in letter_counts_bi_grams:\n",
    "    letter[1] = letter[1] / apperences\n",
    "letter_counts_bi_grams = sorted(letter_counts_bi_grams, key=lambda x:x[1], reverse=True)\n",
    "print(letter_counts_bi_grams[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9144747-c6fa-464e-bd12-b878c17b061c",
   "metadata": {},
   "source": [
    "# Algorithm PianoText\n",
    "## First loop (generate the first mapping of letters and notes by there frequency)\n",
    "\n",
    "This part of the algorithm assigns the n-frequent-letter to the n-frequent-note. For every assignment it then checks for all already mapped letters if there a common bi-grams. If there is a common bi-gram the algorithm searches if the mapping of these two letters is a common transition in music, if it is not a frequent transition it reassigns the letter to the next common note an checks again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d42579c-b6af-4c84-8a44-7e4a9f4dd583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', '71'), ('t', '74'), ('a', '72'), ('o', '69'), ('n', '73'), ('i', '75'), ('s', '76'), ('h', '67'), ('r', '70'), ('d', '77'), ('l', '68'), ('u', '78'), ('c', '79'), ('m', '66'), ('f', '64'), ('w', '65'), ('y', '80'), ('g', '62'), ('p', '63'), ('b', '81'), ('v', '60'), ('k', '82'), ('x', '61'), ('j', '83'), ('q', '86'), ('z', '84')]\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "\n",
    "assig_notes_list = uni_grams.copy()\n",
    "for new_letter in letter_counts:\n",
    "    for note in assig_notes_list:\n",
    "        reassignment = False\n",
    "        assigned_frequency = note[0]\n",
    "        for letter in mapping.keys():\n",
    "            #check if word combinations are common\n",
    "            combination_one = f'{new_letter[0]} - {letter}'\n",
    "            combination_two = f'{letter} - {new_letter[0]}'\n",
    "            for bi_gram_letters in letter_counts_bi_grams:\n",
    "                if(bi_gram_letters[1] < 0.001):\n",
    "                    break\n",
    "                if(combination_one == bi_gram_letters[0]):\n",
    "                    # check if note transition is common\n",
    "                    bi_gram_current_notes = f'[{assigned_frequency}, {mapping[combination_one[-1]]}]'\n",
    "                    reassignment = True\n",
    "                    for bi_gram_notes in bi_grams_transitions:\n",
    "                        if(bi_gram_notes[0] == bi_gram_current_notes):\n",
    "                            reassignment = False   \n",
    "                elif(combination_two == bi_gram_letters[0]):\n",
    "                    bi_gram_current_notes = f'[{mapping[combination_one[-1]]}, {assigned_frequency}]'\n",
    "                    reassignment = True\n",
    "                    for bi_gram_notes in bi_grams_transitions:\n",
    "                        if(bi_gram_notes[0] == bi_gram_current_notes):\n",
    "                            reassignment = False\n",
    "        if not reassignment:\n",
    "            mapping[new_letter[0]] = assigned_frequency\n",
    "            assig_notes_list.remove(note)\n",
    "            break\n",
    "# source https://www.geeksforgeeks.org/python-convert-dictionary-to-list-of-tuples/\n",
    "mapping_list = [(k, v) for k, v in mapping.items()]\n",
    "print(mapping_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa0f33-4512-4a18-b1a5-1460e00844d1",
   "metadata": {},
   "source": [
    "## Second loop (check for common bi-grams)\n",
    "This part of the algorithm goes to all frequent bi-grams and checks the distance of the mapping. If there is a distance greater than 7 semi-tones it assigns the more frequent letter to another note. It starts the search on the opposite side of the infrequent letter to increase the distance to the already existing letter, then it alternates to find the smallest distance to the infrequent letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451f053e-3b91-4c31-8710-152a20db5303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', '71'), ('t', '74'), ('a', '72'), ('o', '69'), ('n', '73'), ('i', '75'), ('s', '76'), ('h', '67'), ('r', '70'), ('d', '77'), ('l', '68'), ('u', '78'), ('c', '79'), ('m', '66'), ('f', '64'), ('w', '65'), ('y', '80'), ('g', '62'), ('p', '63'), ('b', '81'), ('v', '60'), ('k', '82'), ('x', '61'), ('j', '83'), ('q', '86'), ('z', '84'), ('o', 85), ('h', 87), ('t', 88), ('n', 59)]\n"
     ]
    }
   ],
   "source": [
    "taken_positions = []\n",
    "for element in mapping_list:\n",
    "    taken_positions.append(element[1])\n",
    "for bi_gram in letter_counts_bi_grams:\n",
    "    if(bi_gram[1] < 0.01):\n",
    "        break\n",
    "    # get int values of notes\n",
    "    letter_one_two = bi_gram[0].split(' - ')\n",
    "    letter_one = letter_one_two[0]\n",
    "    letter_two = letter_one_two[1]\n",
    "    if(letter_one == letter_two):\n",
    "        continue\n",
    "    # letters now can appeare more then once\n",
    "    apperences_letter_one = []\n",
    "    apperences_letter_two = []\n",
    "    for elements in mapping_list:\n",
    "        if(elements[0] == letter_one):\n",
    "            apperences_letter_one.append(elements[1])\n",
    "        elif(elements[0] == letter_two):\n",
    "            apperences_letter_two.append(elements[1])\n",
    "    # find notes with shortest distance\n",
    "    distance = 10000000\n",
    "    note_one = None\n",
    "    note_two = None\n",
    "    for apperence_one in apperences_letter_one:\n",
    "        for apperence_two in apperences_letter_two:\n",
    "            if(int(apperence_one) - int(apperence_two) < distance):\n",
    "                distance = int(apperence_one) - int(apperence_two)\n",
    "                note_one = int(apperence_one)\n",
    "                note_two = int(apperence_two)\n",
    "    #check distance\n",
    "    if(distance < -7 or 7 < distance):\n",
    "        # remap the more frequent letter\n",
    "        # find the more frequent letter\n",
    "        remap_letter_one = True\n",
    "        for letter in letter_counts:\n",
    "            if(letter == letter_one):\n",
    "                # first letter is more frequent\n",
    "                break\n",
    "            elif(letter == letter_two):\n",
    "                # second letter is more frequent\n",
    "                remap_letter_one = False\n",
    "                break\n",
    "        # remape letter one\n",
    "        if(remap_letter_one):\n",
    "            # if distance is positive note one is higer\n",
    "            # if distance is negative note two is higer\n",
    "            # at first we search on the opposite side to increase the distance to the existing note\n",
    "            # then we search alternating to minimize the distance\n",
    "            starts_positive = True\n",
    "            if(distance < 0):\n",
    "                distance_counter = 1\n",
    "            else:\n",
    "                distance_counter = -1\n",
    "                start_positive = False\n",
    "            while(True):\n",
    "                new_position = note_two + distance_counter\n",
    "                if(f'{new_position}' in taken_positions):\n",
    "                    if(distance_counter < 0):\n",
    "                        if(starts_positive):\n",
    "                            distance_counter -= 1\n",
    "                            distance_counter *= -1\n",
    "                        else:\n",
    "                            distance_counter *= -1\n",
    "                    else:\n",
    "                        if(not starts_positive):\n",
    "                            distance_counter += 1\n",
    "                            distance_counter *= -1\n",
    "                        else:\n",
    "                            distance_counter *= -1\n",
    "                    continue\n",
    "                mapping_list.append((letter_one, new_position))\n",
    "                mapping[letter_one] = f'{new_position}'\n",
    "                taken_positions.append(f'{new_position}')\n",
    "                break\n",
    "        # remap letter two\n",
    "        elif(not remap_letter_one):\n",
    "            # if distance is positive note one is higer\n",
    "            # if distance is negative note two is higer\n",
    "            # at first we search on the opposite side to increase the distance to the existing note\n",
    "            # then we search alternating to minimize the distance\n",
    "            starts_positive = True\n",
    "            if(distance < 0):\n",
    "                distance_counter = -1\n",
    "                starts_positive = False\n",
    "            else:\n",
    "                distance_counter = 1\n",
    "            while(True):\n",
    "                new_position = note_one + distance_counter\n",
    "                if(f'{new_position}' in taken_positions):\n",
    "                    if(distance_counter < 0):\n",
    "                        if(starts_positive):\n",
    "                            distance_counter -= 1\n",
    "                            distance_counter *= -1\n",
    "                        else:\n",
    "                            distance_counter *= -1\n",
    "                    else:\n",
    "                        if(not starts_positive):\n",
    "                            distance_counter -= 1\n",
    "                            distance_counter *= -1\n",
    "                        else:\n",
    "                            distance_counter *= -1\n",
    "                    continue\n",
    "                mapping_list.append((letter_two, new_position))\n",
    "                taken_positions.append(f'{new_position}')\n",
    "                break\n",
    "print(mapping_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
